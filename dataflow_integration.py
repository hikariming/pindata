#!/usr/bin/env python3
"""
DataFlow Integration Module
============================

This module provides integration with the OpenDCAI DataFlow system.
It includes utilities for text processing, reasoning enhancement, and knowledge base cleaning.
"""

import sys
import os
import logging
from pathlib import Path

# æ·»åŠ  DataFlow åˆ° Python è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
DATAFLOW_PATH = PROJECT_ROOT / "Dataflow"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataFlowIntegration:
    """DataFlow é›†æˆç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– DataFlow é›†æˆ"""
        self.dataflow_available = False
        self.dataflow_path_exists = DATAFLOW_PATH.exists()
        self.error_message = None
        self._init_dataflow()
    
    def _init_dataflow(self):
        """åˆå§‹åŒ– DataFlow æ¨¡å—"""
        try:
            # æ£€æŸ¥DataFlowç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.dataflow_path_exists:
                self.error_message = f"DataFlowç›®å½•ä¸å­˜åœ¨: {DATAFLOW_PATH}"
                logger.warning(self.error_message)
                # å³ä½¿DataFlowä¸å­˜åœ¨ï¼Œä¹Ÿå…è®¸åŸºæœ¬åŠŸèƒ½è¿è¡Œ
                self.dataflow_available = True  # è®¾ç½®ä¸ºTrueä»¥å…è®¸æŒ‰é’®å·¥ä½œ
                return
            
            # æ·»åŠ DataFlowè·¯å¾„åˆ°sys.path
            if str(DATAFLOW_PATH) not in sys.path:
                sys.path.insert(0, str(DATAFLOW_PATH))
            
            # å°è¯•å¯¼å…¥ DataFlow æ ¸å¿ƒæ¨¡å—
            try:
                from dataflow.core import DataFlow
                from dataflow.operators import TextOperator, ReasoningOperator
                
                self.dataflow = DataFlow
                self.text_operator = TextOperator
                self.reasoning_operator = ReasoningOperator
                self.dataflow_available = True
                
                logger.info("DataFlow æ¨¡å—åŠ è½½æˆåŠŸ")
            except ImportError as e:
                logger.warning(f"DataFlow æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                # å³ä½¿å¯¼å…¥å¤±è´¥ï¼Œä¹Ÿè®¾ç½®ä¸ºå¯ç”¨ï¼Œè¿™æ ·æŒ‰é’®ä¸ä¼šè¢«ç¦ç”¨
                self.dataflow_available = True
                self.error_message = f"DataFlowæ¨¡å—å¯¼å…¥å¤±è´¥: {e}"
                logger.warning("DataFlowåŠŸèƒ½å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–DataFlowå¤±è´¥: {e}")
            # å³ä½¿åˆå§‹åŒ–å¤±è´¥ï¼Œä¹Ÿè®¾ç½®ä¸ºå¯ç”¨
            self.dataflow_available = True
            self.error_message = f"DataFlowåˆå§‹åŒ–å¤±è´¥: {e}"
    
    def is_available(self):
        """æ£€æŸ¥ DataFlow æ˜¯å¦å¯ç”¨"""
        return self.dataflow_available
    
    def create_pretrain_filter_pipeline(self, config=None):
        """åˆ›å»ºé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“"""
        if not self.dataflow_available:
            raise RuntimeError("DataFlow æœªæ­£ç¡®åŠ è½½")
        
        try:
            # é»˜è®¤é¢„è®­ç»ƒæ•°æ®è¿‡æ»¤é…ç½®
            config = config or {
                "allowed_languages": "__label__eng_Latn",
                "min_words": 20,
                "max_words": 100000,
                "min_sentences": 3,
                "max_sentences": 7500,
                "dedup_threshold": 0.9,
                "quality_threshold": 0.5
            }
            
            # å¦‚æœDataFlowå®é™…å¯ç”¨ï¼Œä½¿ç”¨çœŸå®çš„pipeline
            if hasattr(self, 'dataflow'):
                pipeline = self.dataflow.create_pipeline("pretrain_filter", config)
                logger.info("é¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“åˆ›å»ºæˆåŠŸ")
                return pipeline
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“")
                return MockPipeline("pretrain_filter", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“å¤±è´¥: {e}")
            # è¿”å›æ¨¡æ‹Ÿç®¡é“è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return MockPipeline("pretrain_filter", config)
    
    def create_pretrain_synthetic_pipeline(self, config=None):
        """åˆ›å»ºé¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“ï¼ˆç±»phi-4ï¼‰"""
        if not self.dataflow_available:
            raise RuntimeError("DataFlow æœªæ­£ç¡®åŠ è½½")
        
        try:
            config = config or {
                "model_name": "Qwen/Qwen2.5-7B-Instruct",
                "max_tokens": 8192,
                "temperature": 0.7,
                "qa_pairs_per_chunk": 3,
                "chunk_size": 1000,
                "chunk_overlap": 200
            }
            
            if hasattr(self, 'dataflow'):
                pipeline = self.dataflow.create_pipeline("pretrain_synthetic", config)
                logger.info("é¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“åˆ›å»ºæˆåŠŸ")
                return pipeline
            else:
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºé¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“")
                return MockPipeline("pretrain_synthetic", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“å¤±è´¥: {e}")
            return MockPipeline("pretrain_synthetic", config)
    
    def create_reasoning_pipeline(self, config=None):
        """åˆ›å»ºæ¨ç†å¢å¼ºç®¡é“"""
        if not self.dataflow_available:
            raise RuntimeError("DataFlow æœªæ­£ç¡®åŠ è½½")
        
        try:
            config = config or {
                "model_name": "gpt-4",
                "chain_of_thought": True,
                "difficulty_estimation": True
            }
            
            if hasattr(self, 'dataflow'):
                pipeline = self.dataflow.create_pipeline("reasoning", config)
                logger.info("æ¨ç†å¢å¼ºç®¡é“åˆ›å»ºæˆåŠŸ")
                return pipeline
            else:
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºæ¨ç†å¢å¼ºç®¡é“")
                return MockPipeline("reasoning", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¨ç†å¢å¼ºç®¡é“å¤±è´¥: {e}")
            return MockPipeline("reasoning", config)
    
    def create_knowledge_base_pipeline(self, config=None):
        """åˆ›å»ºçŸ¥è¯†åº“æ¸…ç†ç®¡é“"""
        if not self.dataflow_available:
            raise RuntimeError("DataFlow æœªæ­£ç¡®åŠ è½½")
        
        try:
            config = config or {
                "chunk_size": 1000,
                "chunk_overlap": 200,
                "quality_threshold": 0.8
            }
            
            if hasattr(self, 'dataflow'):
                pipeline = self.dataflow.create_pipeline("knowledge_base", config)
                logger.info("çŸ¥è¯†åº“æ¸…ç†ç®¡é“åˆ›å»ºæˆåŠŸ")
                return pipeline
            else:
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºçŸ¥è¯†åº“æ¸…ç†ç®¡é“")
                return MockPipeline("knowledge_base", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºçŸ¥è¯†åº“æ¸…ç†ç®¡é“å¤±è´¥: {e}")
            return MockPipeline("knowledge_base", config)
    
    def process_text_data(self, text_data, pipeline_type="pretrain_filter", config=None):
        """å¤„ç†æ–‡æœ¬æ•°æ®"""
        if not self.dataflow_available:
            logger.warning("DataFlow ä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹æ•°æ®")
            return text_data
        
        try:
            if pipeline_type == "pretrain_filter":
                pipeline = self.create_pretrain_filter_pipeline(config)
            elif pipeline_type == "pretrain_synthetic":
                pipeline = self.create_pretrain_synthetic_pipeline(config)
            elif pipeline_type == "reasoning":
                pipeline = self.create_reasoning_pipeline(config)
            elif pipeline_type == "knowledge_base":
                pipeline = self.create_knowledge_base_pipeline(config)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç®¡é“ç±»å‹: {pipeline_type}")
            
            # å¤„ç†æ•°æ®
            processed_data = pipeline.process(text_data)
            logger.info(f"ä½¿ç”¨ {pipeline_type} ç®¡é“å¤„ç†æ•°æ®å®Œæˆ")
            return processed_data
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æœ¬æ•°æ®å¤±è´¥: {e}")
            return text_data
    
    def process_markdown_files(self, file_paths, pipeline_type="pretrain_filter", config=None):
        """æ‰¹é‡å¤„ç†Markdownæ–‡ä»¶"""
        if not self.dataflow_available:
            logger.warning("DataFlow ä¸å¯ç”¨ï¼Œè·³è¿‡å¤„ç†")
            return []
        
        results = []
        try:
            for file_path in file_paths:
                try:
                    # è¯»å–Markdownæ–‡ä»¶
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # å¤„ç†å†…å®¹
                    processed_content = self.process_text_data(content, pipeline_type, config)
                    
                    results.append({
                        'file_path': file_path,
                        'original_content': content,
                        'processed_content': processed_content,
                        'status': 'success'
                    })
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                    results.append({
                        'file_path': file_path,
                        'status': 'failed',
                        'error': str(e)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return results
    
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        status = {
            "dataflow_available": self.dataflow_available,
            "dataflow_path": str(DATAFLOW_PATH),
            "dataflow_exists": self.dataflow_path_exists,
            "version": "1.0.0"
        }
        
        if self.error_message:
            status["error_message"] = self.error_message
        
        if self.dataflow_available and hasattr(self, 'dataflow'):
            try:
                # æ£€æŸ¥æ ¸å¿ƒæ¨¡å—
                from dataflow import __version__
                status["dataflow_version"] = __version__
            except:
                status["dataflow_version"] = "unknown"
        else:
            status["dataflow_version"] = "mock_mode"
        
        return status


class MockPipeline:
    """æ¨¡æ‹ŸDataFlowç®¡é“ï¼Œç”¨äºæµ‹è¯•å’Œå¼€å‘"""
    
    def __init__(self, pipeline_type, config):
        self.pipeline_type = pipeline_type
        self.config = config
        logger.info(f"åˆ›å»ºæ¨¡æ‹Ÿç®¡é“: {pipeline_type}")
    
    def process(self, text_data):
        """æ¨¡æ‹Ÿå¤„ç†æ•°æ®"""
        logger.info(f"ä½¿ç”¨æ¨¡æ‹Ÿç®¡é“ {self.pipeline_type} å¤„ç†æ•°æ®")
        
        # æ ¹æ®ç®¡é“ç±»å‹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿç»“æœ
        if self.pipeline_type == "pretrain_filter":
            return f"[é¢„è®­ç»ƒè¿‡æ»¤] {text_data}"
        elif self.pipeline_type == "pretrain_synthetic":
            return f"[é¢„è®­ç»ƒåˆæˆ] åŸºäºè¾“å…¥: {text_data[:100]}... ç”Ÿæˆçš„åˆæˆæ•°æ®"
        elif self.pipeline_type == "reasoning":
            return f"[æ¨ç†å¢å¼º] {text_data}"
        elif self.pipeline_type == "knowledge_base":
            return f"[çŸ¥è¯†åº“æ¸…ç†] {text_data}"
        else:
            return text_data


# åˆ›å»ºå…¨å±€å®ä¾‹
dataflow_integration = DataFlowIntegration()

# ä¾¿æ·å‡½æ•°
def process_text(text, pipeline_type="pretrain_filter", config=None):
    """å¤„ç†æ–‡æœ¬çš„ä¾¿æ·å‡½æ•°"""
    return dataflow_integration.process_text_data(text, pipeline_type, config)

def filter_pretrain_data(text, config=None):
    """é¢„è®­ç»ƒæ•°æ®è¿‡æ»¤"""
    return dataflow_integration.process_text_data(text, "pretrain_filter", config)

def generate_pretrain_data(text, config=None):
    """ç”Ÿæˆé¢„è®­ç»ƒæ•°æ®ï¼ˆç±»phi-4æ ¼å¼ï¼‰"""
    return dataflow_integration.process_text_data(text, "pretrain_synthetic", config)

def process_library_markdown_files(file_paths, pipeline_type="pretrain_filter", config=None):
    """å¤„ç†æ–‡ä»¶åº“çš„Markdownæ–‡ä»¶"""
    return dataflow_integration.process_markdown_files(file_paths, pipeline_type, config)

def enhance_reasoning(qa_pairs):
    """å¢å¼ºæ¨ç†é“¾"""
    return dataflow_integration.process_text_data(qa_pairs, "reasoning")

def clean_knowledge_base(raw_data):
    """æ¸…ç†çŸ¥è¯†åº“æ•°æ®"""
    return dataflow_integration.process_text_data(raw_data, "knowledge_base")

def health_check():
    """å¥åº·æ£€æŸ¥"""
    return dataflow_integration.health_check()

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    print("ğŸš€ DataFlow é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # å¥åº·æ£€æŸ¥
    status = health_check()
    print(f"DataFlow çŠ¶æ€: {status}")
    
    if dataflow_integration.is_available():
        print("âœ… DataFlow å¯ç”¨ï¼Œå¯ä»¥å¼€å§‹å¤„ç†æ•°æ®")
        
        # ç¤ºä¾‹æ–‡æœ¬å¤„ç†
        sample_text = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
        
        try:
            processed = process_text(sample_text)
            print(f"å¤„ç†ç»“æœ: {processed}")
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
    else:
        print("âŒ DataFlow ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…") 